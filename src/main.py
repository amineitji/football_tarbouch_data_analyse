"""
Main - Pipeline complet d'analyse de joueur FBref
Orchestre : Scraping ‚Üí Nettoyage ‚Üí Analyse ‚Üí Visualisation
"""

import os
import sys
from datetime import datetime

# Import des classes custom
from fbref_scraper import FBrefScraper
from data_cleaner import DataCleaner
from player_analyzer import PlayerAnalyzer


def print_header(title: str):
    """Affiche un header stylis√©"""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80)


def print_step(step_number: int, total_steps: int, description: str):
    """Affiche une √©tape du pipeline"""
    print(f"\n{'‚îÄ'*80}")
    print(f"üìç √âTAPE {step_number}/{total_steps} : {description}")
    print(f"{'‚îÄ'*80}")


def main():
    """
    Pipeline complet d'analyse d'un joueur FBref
    """
    
    # ========================================================================
    # CONFIGURATION
    # ========================================================================
    
    PLAYER_CONFIG = {
        'name': 'Marco Verratti',
        'url': 'https://fbref.com/en/players/1467af0d/scout/11454/Marco-Verratti-Scouting-Report',
        'table_id': 'scout_full_MF',
        'position': 'MF'
    }
    
    SCRAPER_CONFIG = {
        'wait_time': 10,
        'headless': True  # False pour voir le navigateur
    }
    
    CLEANING_CONFIG = {
        'remove_composites': True,
        'remove_percentages': True,
        'remove_duplicates': True,
        'remove_empty': True
    }
    
    OUTPUT_DIR = './fbref_analysis_output'
    
    # ========================================================================
    # D√âBUT DU PIPELINE
    # ========================================================================
    
    print_header("üîç PIPELINE D'ANALYSE FBREF")
    print(f"Joueur    : {PLAYER_CONFIG['name']}")
    print(f"Position  : {PLAYER_CONFIG['position']}")
    print(f"Output    : {OUTPUT_DIR}")
    print(f"Timestamp : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Cr√©er le dossier de sortie
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"\n‚úì Dossier de sortie cr√©√© : {OUTPUT_DIR}")
    
    # ========================================================================
    # √âTAPE 1 : SCRAPING
    # ========================================================================
    
    print_step(1, 4, "SCRAPING DES DONN√âES")
    
    scraper = FBrefScraper(
        wait_time=SCRAPER_CONFIG['wait_time'],
        headless=SCRAPER_CONFIG['headless']
    )
    
    try:
        df_raw = scraper.scrape_player(
            url=PLAYER_CONFIG['url'],
            table_id=PLAYER_CONFIG['table_id'],
            player_name=PLAYER_CONFIG['name']
        )
        
        if df_raw is None:
            print("\n‚ùå ERREUR : √âchec du scraping")
            return
        
        print(f"\n‚úì Scraping r√©ussi : {len(df_raw)} lignes extraites")
        
        # Sauvegarder les donn√©es brutes
        raw_file = os.path.join(OUTPUT_DIR, f"{PLAYER_CONFIG['name'].replace(' ', '_')}_raw.csv")
        df_raw.to_csv(raw_file, index=False, encoding='utf-8-sig')
        print(f"‚úì Donn√©es brutes sauvegard√©es : {raw_file}")
        
    finally:
        scraper.close()
    
    # ========================================================================
    # √âTAPE 2 : NETTOYAGE
    # ========================================================================
    
    print_step(2, 4, "NETTOYAGE DES DONN√âES")
    
    cleaner = DataCleaner(verbose=True)
    
    df_clean = cleaner.clean(
        df_raw,
        remove_composites=CLEANING_CONFIG['remove_composites'],
        remove_percentages=CLEANING_CONFIG['remove_percentages'],
        remove_duplicates=CLEANING_CONFIG['remove_duplicates'],
        remove_empty=CLEANING_CONFIG['remove_empty']
    )
    
    print(f"\n‚úì Nettoyage termin√© : {len(df_clean)} lignes conserv√©es")
    
    # Afficher le rapport de nettoyage
    cleaner.print_cleaning_report()
    
    # Sauvegarder les donn√©es nettoy√©es
    clean_file = os.path.join(OUTPUT_DIR, f"{PLAYER_CONFIG['name'].replace(' ', '_')}_clean.csv")
    df_clean.to_csv(clean_file, index=False, encoding='utf-8-sig')
    print(f"\n‚úì Donn√©es nettoy√©es sauvegard√©es : {clean_file}")
    
    # ========================================================================
    # √âTAPE 3 : ANALYSE TEXTUELLE
    # ========================================================================
    
    print_step(3, 4, "ANALYSE DU JOUEUR")
    
    # Copier les m√©tadonn√©es
    df_clean.attrs = df_raw.attrs.copy()
    
    analyzer = PlayerAnalyzer(
        player_name=PLAYER_CONFIG['name'],
        position=PLAYER_CONFIG['position']
    )
    
    analyzer.load_data(df_clean)
    
    # Afficher le profil textuel
    analyzer.print_player_profile()
    
    # Sauvegarder le r√©sum√© textuel
    summary_file = os.path.join(OUTPUT_DIR, f"{PLAYER_CONFIG['name'].replace(' ', '_')}_summary.txt")
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        # Rediriger la sortie vers le fichier
        old_stdout = sys.stdout
        sys.stdout = f
        
        analyzer.print_player_profile()
        
        sys.stdout = old_stdout
    
    print(f"\n‚úì R√©sum√© textuel sauvegard√© : {summary_file}")
    
    # ========================================================================
    # √âTAPE 4 : VISUALISATIONS
    # ========================================================================
    
    print_step(4, 4, "G√âN√âRATION DES VISUALISATIONS")
    
    # Cr√©er un sous-dossier pour les graphiques
    viz_dir = os.path.join(OUTPUT_DIR, 'visualizations')
    os.makedirs(viz_dir, exist_ok=True)
    
    print("\nG√©n√©ration des graphiques...")
    
    safe_name = PLAYER_CONFIG['name'].replace(' ', '_').replace('-', '_')
    
    # 1. Radar des percentiles
    print("  [1/4] Radar des percentiles...")
    analyzer.plot_percentile_radar(
        save_path=os.path.join(viz_dir, f'{safe_name}_radar.png')
    )
    
    # 2. Top statistiques
    print("  [2/4] Top statistiques...")
    analyzer.plot_top_stats(
        n=15,
        save_path=os.path.join(viz_dir, f'{safe_name}_top_stats.png')
    )
    
    # 3. Comparaison par cat√©gories
    print("  [3/4] Comparaison par cat√©gories...")
    analyzer.plot_category_comparison(
        save_path=os.path.join(viz_dir, f'{safe_name}_categories.png')
    )
    
    # 4. Distribution des percentiles
    print("  [4/4] Distribution des percentiles...")
    analyzer.plot_percentile_distribution(
        save_path=os.path.join(viz_dir, f'{safe_name}_distribution.png')
    )
    
    print(f"\n‚úì Visualisations sauvegard√©es dans : {viz_dir}")
    
    # ========================================================================
    # R√âSUM√â FINAL
    # ========================================================================
    
    print_header("üìä PIPELINE TERMIN√â")
    
    print("\nüìÅ Fichiers g√©n√©r√©s :")
    print(f"  ‚Ä¢ Donn√©es brutes      : {raw_file}")
    print(f"  ‚Ä¢ Donn√©es nettoy√©es   : {clean_file}")
    print(f"  ‚Ä¢ R√©sum√© textuel      : {summary_file}")
    print(f"  ‚Ä¢ Visualisations      : {viz_dir}/")
    
    print("\nüé® Graphiques cr√©√©s :")
    print(f"  ‚Ä¢ Radar des percentiles")
    print(f"  ‚Ä¢ Top 15 statistiques")
    print(f"  ‚Ä¢ Comparaison par cat√©gories")
    print(f"  ‚Ä¢ Distribution des percentiles")
    
    print("\n‚úÖ Analyse compl√®te termin√©e avec succ√®s !")
    print(f"üìÇ Tous les fichiers sont dans : {OUTPUT_DIR}")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Pipeline interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n\n‚ùå ERREUR FATALE : {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nüëã Fin du programme")